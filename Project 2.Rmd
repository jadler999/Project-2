---
title: 'Project #2'
author: "J.Bonacci, Stefan Jonsson, James Adler, Nikki Barden"
date: "5/15/2018"
output: html_document
---

```{r}
remove(list=ls())
```

## R Markdown

##Libraries
```{r}
library('ddalpha')
library('kernlab')
library('caret')
library('MASS')
library('randomForest')
library("dplyr")
library("e1071")
library('rpart')
library('rpart.plot')
```


#Reading in the datasets 
```{r}
train_data <- read.csv('https://raw.githubusercontent.com/slevkoff/ECON386REPO/master/Prediction%20Project/training.csv')
test_data <- read.csv('https://raw.githubusercontent.com/slevkoff/ECON386REPO/master/Prediction%20Project/testing.csv')
```

##Count NA values
```{r}
sapply(train_data, function(x) sum(is.na(x)))
sapply(test_data, function(x) sum(is.na(x)))
```

#Remove all columns containing at least one NA
```{r}
train_data2 <- train_data[ , apply(train_data, 2, function(x) !any(is.na(x)))]
test_data2 <- test_data[ , apply(test_data, 2, function(x) !any(is.na(x)))]
```

#input NAs into all blank observations
```{r}
train_data2[train_data2==""] <- NA
test_data2[test_data2==""] <- NA

```
#Count NA values again to check

```{r}
sapply(train_data2, function(x) sum(is.na(x)))
sapply(test_data2, function(x) sum(is.na(x)))
```

#input NAs into all blank observations
```{r}
train_data3<- train_data2[ , apply(train_data2, 2, function(x) !any(is.na(x)))]
test_data3<- test_data2[ , apply(test_data2, 2, function(x) !any(is.na(x)))]
```

#omits zero variance predictors
```{r}
##freq cut and unique cut arguments can be ommitted if it fits better with out them (leaving arguments in cuts more predictors)

remove_cols <- nearZeroVar(train_data3,names=TRUE)
all_cols<-names(train_data3)
train_data4<-train_data3[ , setdiff(all_cols,remove_cols)]



remove_cols2<-nearZeroVar(test_data3,names=TRUE)
all_cols2<-names(test_data3)
test_data4<-test_data3[ , setdiff(all_cols2,remove_cols2)]




```

#rename datasets
```{r}
train <- train_data4
test <- test_data4
```

#removing timestamps and factor variables
```{r}
train_final<- train[c(7:59)]
test_final<- test[c(7:59)]
```



#partitioning data
```{r}
#partitions 70% of data into training set
trainingRowIndex<-sample(1:nrow(train_final), size = .7*nrow(train_final))
part_training<-train_final[trainingRowIndex, ]

#leaves 30% for testing and validating 
part_test <-train_final[-trainingRowIndex, ]



```

#removing old datasets
```{r}
remove(train_data, train_data2, train_data3,train_data4, train, test_data, test_data2, test_data3, test,test_data4,train_final)
```

#random forest with default number of variables at each node (Jack Bonacci)
```{r}

set.seed(1234)
part_training$classe<- as.factor(part_training$classe)
jack_rf<- randomForest(classe~., part_training)
jack_pred<- predict(jack_rf, part_test)
confusionMatrix(jack_pred, part_test$classe,  dnn = c("Prediction", "Reference"))




#Running algorithm on test data
jack_final_prediction<- predict(jack_rf, test_final)
jack_final_prediction
```


#cart (James Adler)
```{r}
#creating model and looking at it
set.seed(117)
james_tree <- rpart(classe ~. ,data = part_training, method = "class")
printcp(james_tree)
rpart.plot(james_tree,type = 3,digits = 3, fallen.leaves = TRUE)

#runs new algorithm on new data
james_pred<-predict(james_tree, part_test,type="class")
confusionMatrix(james_pred,part_test$classe)


james_final_prediction<-predict(james_tree,test_final)
james_final_prediction


```

#Random forest variation with regularization of data (Nikki)
```{r}
set.seed(1234)
x <- subset(part_training, select=-classe)
y <- part_training$classe
bestmtry <- tuneRF(x, y, stepFactor = 1.5, improve=1e-5, ntree=500)
print(bestmtry)
nikki_rf <- randomForest(classe~., data=part_training, ntree=300, mtry=10, importance=TRUE)
nikki_pred <- predict(nikki_rf, part_test)
nikki_pred
confusionMatrix(nikki_pred, part_test$classe, dnn=c("prediction","reference"))


# Run final prediction
nikki_finalpred <- predict(nikki_rf, test_final)
nikki_finalpred
```


