---
title: 'Project #2'
author: "J.Bonacci, Stefan Jonsson, James Adler, Nikki Barden"
date: "5/15/2018"
output:
  pdf_document: default
  html_document: default
---

```{r}
remove(list=ls())
```

## R Markdown
##Nikki's method consistently performed the best so we chose to go with hers. We tried to cut our predictors by making the non zero variance function stricter but it actually hurt our prediction out of sample. We chose not to validate the data because it did not improve the random forest model much or at all. Nikki_finalpred is the final output for the out of sample test set.

##Libraries
```{r}
library('ddalpha')
library('kernlab')
library('caret')
library('MASS')
library('randomForest')
library("dplyr")
library("e1071")
library('rpart')
library('rpart.plot')
library('neuralnet')
library('nnet')
library('ggvis')
library('class')
library('gmodels')
```


#Reading in the datasets 
```{r}
train_data <- read.csv('https://raw.githubusercontent.com/slevkoff/ECON386REPO/master/Prediction%20Project/training.csv')
test_data <- read.csv('https://raw.githubusercontent.com/slevkoff/ECON386REPO/master/Prediction%20Project/testing.csv')
```

##Count NA values
```{r}
sapply(train_data, function(x) sum(is.na(x)))
sapply(test_data, function(x) sum(is.na(x)))
```

#Remove all columns containing at least one NA
```{r}
train_data2 <- train_data[ , apply(train_data, 2, function(x) !any(is.na(x)))]
test_data2 <- test_data[ , apply(test_data, 2, function(x) !any(is.na(x)))]
```

#input NAs into all blank observations
```{r}
train_data2[train_data2==""] <- NA
test_data2[test_data2==""] <- NA

```
#Count NA values again to check

```{r}
sapply(train_data2, function(x) sum(is.na(x)))
sapply(test_data2, function(x) sum(is.na(x)))
```

#input NAs into all blank observations
```{r}
train_data3<- train_data2[ , apply(train_data2, 2, function(x) !any(is.na(x)))]
test_data3<- test_data2[ , apply(test_data2, 2, function(x) !any(is.na(x)))]
```

#omits zero variance predictors
```{r}
##freq cut and unique cut arguments can be ommitted if it fits better with out them (leaving arguments in cuts more predictors)

remove_cols <- nearZeroVar(train_data3,names=TRUE)
all_cols<-names(train_data3)
train_data4<-train_data3[ , setdiff(all_cols,remove_cols)]



remove_cols2<-nearZeroVar(test_data3,names=TRUE)
all_cols2<-names(test_data3)
test_data4<-test_data3[ , setdiff(all_cols2,remove_cols2)]




```

#rename datasets
```{r}
train <- train_data4
test <- test_data4
```

#removing timestamps and factor variables
```{r}
train_final<- train[c(7:59)]
test_final<- test[c(7:59)]
```



#partitioning data
```{r}
#partitions 70% of data into training set
trainingRowIndex<-sample(1:nrow(train_final), size = .7*nrow(train_final))
part_training<-train_final[trainingRowIndex, ]

#leaves 30% for testing and validating 
part_test <-train_final[-trainingRowIndex, ]



```

#removing old datasets
```{r}
remove(train_data, train_data2, train_data3,train_data4, train, test_data, test_data2, test_data3, test,test_data4,train_final)
```

#random forest with default number of variables at each node (Jack Bonacci)
```{r}

set.seed(1234)
#creates the random forest
part_training$classe<- as.factor(part_training$classe)
jack_rf<- randomForest(classe~., part_training)
jack_pred<- predict(jack_rf, part_test)
confusionMatrix(jack_pred, part_test$classe,  dnn = c("Prediction", "Reference"))




#Running algorithm on test data
jack_final_prediction<- predict(jack_rf, test_final)
jack_final_prediction
```


#cart (James Adler)
```{r}
#creating cart model 
set.seed(117)
james_tree <- rpart(classe ~. ,data = part_training, method = "class")
#gets the optimal cp (cp with minimum erro)
optimal_cp<-james_tree$cptable[which.min(james_tree$cptable[,"xerror"]),"CP"]
#uses optimal cp to create new tree
james_ptree<-prune(james_tree,optimal_cp)
#viewing new tree
rpart.plot(james_ptree,type = 3,digits = 3, fallen.leaves = TRUE)
#runs new pruned tree model with partitioned test data
james_pred<-predict(james_ptree, part_test, type="class")
confusionMatrix(james_pred,part_test$classe)


#tests our algorithm on the 20 outputless observations
james_final_prediction<-predict(james_ptree,test_final)
james_final_prediction


```

#Random forest variation with regularization of data (Nikki)
```{r}
set.seed(1234)
x <- subset(part_training, select=-classe)
y <- part_training$classe
bestmtry <- tuneRF(x, y, stepFactor = 1.5, improve=1e-5, ntree=500)
print(bestmtry)
nikki_rf <- randomForest(classe~., data=part_training, ntree=300, mtry=10, importance=TRUE)
nikki_pred <- predict(nikki_rf, part_test)
nikki_pred
confusionMatrix(nikki_pred, part_test$classe, dnn=c("prediction","reference"))


# Run final prediction
nikki_finalpred <- predict(nikki_rf, test_final)
nikki_finalpred
```


#Support Vector Machine by Stefan (we attempted to tune it but the accuracy only minorly improved and it was quite time consuming)
```{r}
stefan_svm <- svm(classe ~. ,data = part_training)
stefan_pred<- predict(stefan_svm, part_test)
confusionMatrix(stefan_pred,part_test$classe, dnn=c("prediction","reference"))


```





#This is Stefan's neural network enter if you dare my computer hates it

scaledata<-scale(train_final[,1:52]) #scale normalization

normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}#max-min normalization

maxmindf<-as.data.frame(lapply(scaledata, normalize))#normilization takes too much computing power

test_final_2<-cbind(maxmindf,class.ind(train_final$classe))

#partitions 70% of data into training set
trainingRowIndex<-sample(1:nrow(train_final2), size = .7*nrow(train_final2))
trainset<-train_final2[trainingRowIndex, ]

#leaves 30% for testing and validating 
testset<-train_final2[-trainingRowIndex, ]


library(neuralnet) #Neural Network
nn <- neuralnet(A + B + C + D + E ~ roll_belt+pitch_belt+yaw_belt+total_accel_belt+gyros_belt_x+gyros_belt_y
                  +gyros_belt_z+accel_belt_x+accel_belt_y+accel_belt_z+magnet_belt_x+magnet_belt_y
                  +magnet_belt_z+roll_arm+pitch_arm+yaw_arm+total_accel_arm+gyros_arm_x+gyros_arm_y
                  +gyros_arm_z+accel_arm_x+accel_arm_y+accel_arm_z+magnet_arm_x+magnet_arm_y+magnet_arm_z
                  +roll_dumbbell+pitch_dumbbell+yaw_dumbbell+total_accel_dumbbell+gyros_dumbbell_x
                  +gyros_dumbbell_y+gyros_dumbbell_z+accel_dumbbell_x+accel_dumbbell_y+accel_dumbbell_z
                  +magnet_dumbbell_x+magnet_dumbbell_y+magnet_dumbbell_z+roll_forearm+pitch_forearm
                  +yaw_forearm+total_accel_forearm+gyros_forearm_x+gyros_forearm_y+gyros_forearm_z
                  +accel_forearm_x+accel_forearm_y+accel_forearm_z+magnet_forearm_x+magnet_forearm_y
                  +magnet_forearm_z, data=test_final_2, hidden=c(10,5),linear.output=FALSE, threshold=0.01)
nn$result.matrix
plot(nn)

#Test the resulting output
temp_test <- subset(testset, select = c("roll_belt","pitch_belt", "yaw_belt","total_accel_belt","gyros_belt_x","gyros_belt_y","gyros_belt_z","accel_belt_x","accel_belt_y","accel_belt_z","magnet_belt_x","magnet_belt_y","magnet_belt_z","roll_arm","pitch_arm","yaw_arm","total_accel_arm","gyros_arm_x","gyros_arm_y","gyros_arm_z","accel_arm_x","accel_arm_y","accel_arm_z","magnet_arm_x","magnet_arm_y","magnet_arm_z","roll_dumbbell","pitch_dumbbell","yaw_dumbbell","total_accel_dumbbell","gyros_dumbbell_x","gyros_dumbbell_y","gyros_dumbbell_z","accel_dumbbell_x","accel_dumbbell_y","accel_dumbbell_z","magnet_dumbbell_x","magnet_dumbbell_y","magnet_dumbbell_z","roll_forearm","pitch_forearm","yaw_forearm","total_accel_forearm","gyros_forearm_x","gyros_forearm_y","gyros_forearm_z","accel_forearm_x","accel_forearm_y","accel_forearm_z","magnet_forearm_x","magnet_forearm_y","magnet_forearm_z"))

head(temp_test)

nn.results <- compute(nn, temp_test)

#Accuracy
results <- data.frame(actual = testset("A","B","C","D","E"), prediction = nn.results$net.result)

results
roundedresults<-sapply(results,round,digits=0)
roundedresultsdf=data.frame(roundedresults)
attach(roundedresultsdf)
table(actual,prediction)
